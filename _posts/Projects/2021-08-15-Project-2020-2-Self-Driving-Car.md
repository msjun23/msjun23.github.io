---
title: "Self Driving Car with Jetson Nano"
excerpt: "2020 하반기 바람 프로젝트"
toc: true
toc_sticky: true
categories:
  - Projects
tag:
  - Projects
  - BARAM
  - Deep Learning
  - Detection
  - Ubuntu
  - Autonomous Driving
last_modified_at: 2021-08-15T01:30-02:00
---

**프로젝트 링크:** [github](https://github.com/msjun23/Self-Driving-Car-Minimi) <br>
**진행 기간:** 2020.9월 ~ 11월

---

# Introduction & Motive

딥러닝과 영상 처리(Image processing)에 관심을 가지게 되고, 이를 예전부터 꿈이었던 로봇에 적용해보고자 했다. 그렇게 가장 먼저 떠오른 것이 자율주행 자동차였다. 물론 기존의 자동차를 로봇이라고 하지는 않지만, 자율 주행 단계를 점차 올려간다면 이는 점차 로봇에 가까워지지 않을까란 생각을 한다.

![auto_level](/assets/images/self-driving-car/auto_level.jpg)

딴 얘기지만 현재 시중에 출시되는 자율주행 자동차는 거의 2~3레벨로 알고 있다. 그리고 법률적인 이유에서인지 위험에 대한 책임소재 때문인지 4~5단계의 고레벨 자율주행을 시도하는 것은 오히려 자동차 회사가 아닌 Google(현재는 거의 철수한 것으로 알고 있지만.), 네이버, NVIDIA 등 IT계열 공룡기업으로 알고 있다.(물론 Tesla와 같은 도전적인 자동차 기업도 있다. 그리고 이 문단은 뇌피셜 성질이 강하므로 무조건적인 수용은 안해주었으면 한다.)

이쯤에서 각설하고, 아무튼 근래에 생긴 관심 영역과 기존의 관심 영역을 융합한 무언가를 해보고 싶었다.

> 모바일 로봇 + 영상 처리(차선 인식) + 딥러닝, 객체 인식(사람, 자동차 인식)<br>
> = 자율주행 자동차

라는 컨셉으로 이 프로젝트를 진행하기로 했다.

# System Architecture

![sys_arch](/assets/images/self-driving-car/sys_arch.png)

# Implementation

영상 처리 및 주 프로세싱은 Jetson Nano 보드에서 수행했다. Jetson Nano에 USB 캠을 통해 전방 이미지를 받아오고, ROI(Region Of Interest)를 설정하여 차선을 인식한다. 동시에 Tensorflow API를 사용해 전방에 존재하는 사람이나 자동차를 인식했다.

![slide4](/assets/images/self-driving-car/slide4.png)

그 후, 차량 제어를 위해 양측 차선을 인식하여, 두 차선의 가로축 중앙 픽셀을 목표지점으로 정하고, 캠 이미지의 가로축 중앙 픽셀을 현재 로봇의 위치로 정하여 두 픽셀 값의 차이를 오차로 정의했다.

해당 값을 시리얼 통신을 통해 모터 제어용 Embedded 보드인 F401RE 보드로 전달했고, 주어진 오차값을 PID 제어기의 입력으로 사용해 모터 제어를 수행했다.

# Result

![output2](/assets/images/self-driving-car/output2.gif)

# Epilogue

당시에는 나름 열심히 한다고 했던 것 같은데 돌아보니 매우 많이 부족했다. 나름의 변명을 해보자면 애초에 너무 대단한 프로젝트를 하려고 했던 것 같고, 어떻게든 퍼포먼스를 보이는 것에만 치중한 것 같다.

할 말이 많지만 하나씩 해보자면, 먼저 제어를 하겠다고 무지성으로 PID 제어기를 쓴 것이다. 사실 제어기를 쓴 건지 그냥 코드로 미적분 공식만 구현한 건지도 모르겠다. 물론 지금도 제어를 안다고 말할 수는 없지만 조금더 배운 나로써 정말 조금만 피드백을 해보자면, 사실 P제어나 PD제어만으로도 충분했을 거란 생각이 든다. 당시 내가 하고자 했던 것은 현재 위치와 타겟 위치의 오차를 사용한 위치 제어인데, 여기에 적분항이 들어갈 이유가 없다. 오히려 위치 오차가 누적되면 시스템이 발산해 버릴지도 모르는 상황이다. 또한 영상 처리 결과의 프레임도 높지 않은 상황에서 빠른 수렴을 위해 적분항을 추가하겠다는 것도 말이 안된다. 당시의 나는 모터만 굴리면 제어인 줄 알았던게 아닐까라는 생각을 해본다.

다음으로 차선 인식에 대한 부분이다. 여기에서 모터 제어 이전의 큰 문제가 하나 존재한다. 현재 카메라가 주시하는 영역은 차량 전방의 영역이다. 즉, 현재 카메라의 위치나 모터의 위치 혹은 차체의 중심 위치와는 전혀 다른, 일정 거리만큼 떨어진 "미래에 위치할" 차선을 보고 있다는 말이다. 현재 자동차가 존재하는 차선은 직선구간이고, 현재 카메라고 보고있는 "조금 앞선 구간"은 곡선구간이라 가정해보자. 그럼 현재 자동차는 직진을 해야하지만, 영상 인식 결과 좌회전이든 우회전이든 회전을 해야한다는 결과가 도출된다. 앞선 문단에서 언급한 제어 이전에 그냥 모든 상황 자체가 거짓일 수 있다는 것이다.

당시에는 이 문제를 아예 의식하지 못했다. 단순히 ROI를 최대한 짧게 잡아, 너무 앞선 구간을 ROI에서 아예 배제하고, 현재 차체의 위치와 "조금 앞선 구간" 사이의 거리를 최소하여 어떻게든 차선을 추종할 수 있었던 것 같다. 그리고 카메라를 하단으로 최대한 틸트시켜 그 오차를 무의식 중에 많이 줄였던 것 같다.

또 한가지는 상위 제어기, 하위 제어기라는 개념이 없었다. 제어를 몰랐다를 앞선 문제와 어느정도 통하는 내용이라 생각한다. Pure pursuit, kanayama 같은 경로 추종 제어 방법론을 전혀 모르고 차선 오차만 있으면 차선을 따라갈 수 있다고 생각했다.

정말 많은 공부가 필요하다는 것을, 이 포스팅을 하면서 다시 한 번 느낄 수 있었다. 많은 부족함이 있었던 작품이었지만 그래도 지나 생각해보면 내가 발전할 수 있는 계기가 되어줬다고 생각한다.
